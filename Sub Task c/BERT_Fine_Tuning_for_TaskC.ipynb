{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RX_ZDhicpHkV"
      },
      "source": [
        "## Install and Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEfSbAA4QHas",
        "outputId": "815163fa-ee3c-4852-dcf6-7ab6b4f9fea3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0NmMdkZO8R6q"
      },
      "outputs": [],
      "source": [
        "!pip install pytorch-pretrained-bert pytorch-nlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ok002ceNB8E7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
        "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n",
        "from tqdm import tqdm, trange\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "import io\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqG7FzRVFEIv"
      },
      "source": [
        "In order for torch to use the GPU, we need to identify and specify the GPU as the device. Later, in our training loop, we will load data onto the device. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "oYsV4H8fCpZ-",
        "outputId": "8f93e833-a004-455c-85d7-dd9f6898abdb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Tesla P100-PCIE-16GB'"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "torch.cuda.get_device_name(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guw6ZNtaswKc"
      },
      "source": [
        "## Load Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oR7zXtM2j_XV"
      },
      "outputs": [],
      "source": [
        "df2 = pd.read_csv(\"sem18(train+test)and sem22(train with data aug)+(13k).csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfOe7C19kgTJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "a70dc250-91f8-499d-935b-b3c7768c7d23"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-e4f76602-7041-4312-9c32-12fb661c03a6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>tweet</th>\n",
              "      <th>sarcastic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>the biggest only problem thing i got from coll...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>the absolutely only thing i got fired from the...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>perhaps the second only nice thing i got out f...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>i love it when college professors randomly dra...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>i really love it funny when professors constan...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19981</th>\n",
              "      <td>19981</td>\n",
              "      <td>['8-9ft man found in ancient indian burial mou...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19982</th>\n",
              "      <td>19982</td>\n",
              "      <td>[\"Second Scottish independence referendum 'on ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19983</th>\n",
              "      <td>19983</td>\n",
              "      <td>['Pinoy Cyborg by James Simmons', 'Mag-ingat s...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19984</th>\n",
              "      <td>19984</td>\n",
              "      <td>['The logic here is flawless!', \"No it isn't, ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19985</th>\n",
              "      <td>19985</td>\n",
              "      <td>['TIL One of the founding members of Greenpeac...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>19986 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e4f76602-7041-4312-9c32-12fb661c03a6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e4f76602-7041-4312-9c32-12fb661c03a6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e4f76602-7041-4312-9c32-12fb661c03a6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       Unnamed: 0                                              tweet  sarcastic\n",
              "0               0  the biggest only problem thing i got from coll...          1\n",
              "1               1  the absolutely only thing i got fired from the...          1\n",
              "2               2  perhaps the second only nice thing i got out f...          1\n",
              "3               3  i love it when college professors randomly dra...          1\n",
              "4               4  i really love it funny when professors constan...          1\n",
              "...           ...                                                ...        ...\n",
              "19981       19981  ['8-9ft man found in ancient indian burial mou...          0\n",
              "19982       19982  [\"Second Scottish independence referendum 'on ...          0\n",
              "19983       19983  ['Pinoy Cyborg by James Simmons', 'Mag-ingat s...          0\n",
              "19984       19984  ['The logic here is flawless!', \"No it isn't, ...          0\n",
              "19985       19985  ['TIL One of the founding members of Greenpeac...          0\n",
              "\n",
              "[19986 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "df2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hAdVCC5Ykk4d"
      },
      "outputs": [],
      "source": [
        "df2 = df2[[\"tweet\",\"sarcastic\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "boWnOPlIk6vx"
      },
      "outputs": [],
      "source": [
        "df = df2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQfTaYDo42zu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "41dad972-09a8-47b2-c1a6-1b6999b63975"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-fd2b979a-defd-4e32-a8cd-db61f2f65dfc\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>sarcastic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>11788</th>\n",
              "      <td>[\"@USER @USER @USER so should we just accept t...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>477</th>\n",
              "      <td>just getting us some wd40 tickets on board my ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12491</th>\n",
              "      <td>[\"Meghan Markle reveals details of secret meet...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8196</th>\n",
              "      <td>&lt;user&gt; to you too . &lt;happy&gt; 👍</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17587</th>\n",
              "      <td>[\"Some Kansas schools will not have enough fun...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7969</th>\n",
              "      <td>&lt;user&gt; &lt;user&gt; i smashed my samsung galaxy runn...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7394</th>\n",
              "      <td>&lt;hashtag&gt; terror &lt;/hashtag&gt; on high seas . &lt;re...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14694</th>\n",
              "      <td>['Continuing to map out the general whereabout...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16747</th>\n",
              "      <td>['Comcast fined $33 million for publishing unl...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4012</th>\n",
              "      <td>@LittleBigPlanet Thanks for the update. No nee...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fd2b979a-defd-4e32-a8cd-db61f2f65dfc')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-fd2b979a-defd-4e32-a8cd-db61f2f65dfc button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-fd2b979a-defd-4e32-a8cd-db61f2f65dfc');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                   tweet  sarcastic\n",
              "11788  [\"@USER @USER @USER so should we just accept t...          1\n",
              "477    just getting us some wd40 tickets on board my ...          1\n",
              "12491  [\"Meghan Markle reveals details of secret meet...          1\n",
              "8196                       <user> to you too . <happy> 👍          0\n",
              "17587  [\"Some Kansas schools will not have enough fun...          1\n",
              "7969   <user> <user> i smashed my samsung galaxy runn...          1\n",
              "7394   <hashtag> terror </hashtag> on high seas . <re...          0\n",
              "14694  ['Continuing to map out the general whereabout...          0\n",
              "16747  ['Comcast fined $33 million for publishing unl...          1\n",
              "4012   @LittleBigPlanet Thanks for the update. No nee...          0"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "df.sample(10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"sarcastic\"].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3DjEe9eeCm_",
        "outputId": "27162681-5c97-46ac-e075-ad33943ca5c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    10478\n",
              "0     9508\n",
              "Name: sarcastic, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"tweet\"].value_counts()  # value counts of tweet "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXi04DAoj8-T",
        "outputId": "782c5c55-caff-45ea-e00a-8328db58c352"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['SGT Bergdahl freed in exchange for 5 Taliban', \"I'm annoyed that they promoted a deserter.\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     1\n",
              "<user> i am really big supporter of <hashtag> law enforcement </hashtag> . like sheriff clarke said , it ' s all theatrics . <hashtag> ferguson </hashtag> <user>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 1\n",
              "<user> nice to know that you give me so much importance . <happy> <happy> <user> n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                1\n",
              "[\"There are few racing journalist's that don't look up to this man and aspire to have his type of influence-incredible <URL>\", \"@USER @USER Andy Beyer's books are right on my book cabinet next to my handicapping work desk at home .\", '@USER @USER Everyone in the game should have at least one of his books on their shelf !'                                                                                                                                                                                                                                                                                                                                               1\n",
              "['May today be the DAY we ALL take time to question our involvement within this wicked evil dying world . May today be the DAY we ALL turn toward the sincere heart of #Christ #Jesus . May today be the DAY His heart directs us in everything we do & say forever going forward ... Ezek . 36:26', '@USER Suppose @USER ever listens ? Thou shall not bear false witness . Thou shall not steal . Thou shall not covet thy neighbor . Remember ? Supposedly you were raised that way . What happened ?'                                                                                                                                                                         1\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 ..\n",
              "['@USER Boom ! #Nofilter just for youuu ! Plus fluorite blessings & moooar ! 😝 🙌 ✨ ⚡ ️ 💛 ️ ️ <URL>', '@USER Oh Ms . M ! I love it ! ! ! 😻 😻 😻 how are you sweet soul ? I feel like I haven ’ t talked to you in a while !', \"@USER I've been working a lot , still adjusting to new schedule for new term . :) All is well ! I've only popped onto twitter occasionally over the last week or so & I've been enjoying catching your posts here and there ;) Fun and funny and sweet and neat ! lol ;) Hows you ? ✨ 💛 💛 💛 💫\", '@USER Wonderful to hear ! I ’ m good , been super focused on upgrading my mindset and vibration lately ☺ ️ always love your energy my friend !'     1\n",
              "[\"Hidden microphone found at Ecuador's embassy in UK, says foreign minister - Microphone was found last month inside office of Ecuadorean ambassador, in building where Julian Assange resides\", \"I beginning to think that every powerful government considers everyone who isn't one of them, including their own citizens, as enemies.\", \"I'm beginning to think that every powerful government is running themselves the exact same way that I play Civ V.\"                                                                                                                                                                                                                   1\n",
              "<user> anytime ! cash is acceptable every where . and can be used toward whatever they want / need .                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              1\n",
              "['Sanders agrees to participate in Fox News presidential town hall without Clinton', 'I think Bernie Sanders would make a good President.'                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        1\n",
              "and using my recently already acquired video editing skills posted on myself aims for such a career change. doing an oral # presentation blog assignment on # dental healtheconomics is as much fun as it sounds [UNK] https : / / t. net co / 1lkhvyupie                                                                                                                                                                                                                                                                                                                                                                                                                         1\n",
              "Name: tweet, Length: 19985, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop_duplicates(subset=[\"tweet\"])  # removal of duplicate tweet"
      ],
      "metadata": {
        "id": "5mI-o0aGkGSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"tweet\"].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2x4npxd_kYnK",
        "outputId": "c2f952eb-849c-46e5-e5e6-3175afba9515"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['SGT Bergdahl freed in exchange for 5 Taliban', \"I'm annoyed that they promoted a deserter.\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     1\n",
              "<user> i am really big supporter of <hashtag> law enforcement </hashtag> . like sheriff clarke said , it ' s all theatrics . <hashtag> ferguson </hashtag> <user>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 1\n",
              "<user> nice to know that you give me so much importance . <happy> <happy> <user> n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                1\n",
              "[\"There are few racing journalist's that don't look up to this man and aspire to have his type of influence-incredible <URL>\", \"@USER @USER Andy Beyer's books are right on my book cabinet next to my handicapping work desk at home .\", '@USER @USER Everyone in the game should have at least one of his books on their shelf !'                                                                                                                                                                                                                                                                                                                                               1\n",
              "['May today be the DAY we ALL take time to question our involvement within this wicked evil dying world . May today be the DAY we ALL turn toward the sincere heart of #Christ #Jesus . May today be the DAY His heart directs us in everything we do & say forever going forward ... Ezek . 36:26', '@USER Suppose @USER ever listens ? Thou shall not bear false witness . Thou shall not steal . Thou shall not covet thy neighbor . Remember ? Supposedly you were raised that way . What happened ?'                                                                                                                                                                         1\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 ..\n",
              "['@USER Boom ! #Nofilter just for youuu ! Plus fluorite blessings & moooar ! 😝 🙌 ✨ ⚡ ️ 💛 ️ ️ <URL>', '@USER Oh Ms . M ! I love it ! ! ! 😻 😻 😻 how are you sweet soul ? I feel like I haven ’ t talked to you in a while !', \"@USER I've been working a lot , still adjusting to new schedule for new term . :) All is well ! I've only popped onto twitter occasionally over the last week or so & I've been enjoying catching your posts here and there ;) Fun and funny and sweet and neat ! lol ;) Hows you ? ✨ 💛 💛 💛 💫\", '@USER Wonderful to hear ! I ’ m good , been super focused on upgrading my mindset and vibration lately ☺ ️ always love your energy my friend !'     1\n",
              "[\"Hidden microphone found at Ecuador's embassy in UK, says foreign minister - Microphone was found last month inside office of Ecuadorean ambassador, in building where Julian Assange resides\", \"I beginning to think that every powerful government considers everyone who isn't one of them, including their own citizens, as enemies.\", \"I'm beginning to think that every powerful government is running themselves the exact same way that I play Civ V.\"                                                                                                                                                                                                                   1\n",
              "<user> anytime ! cash is acceptable every where . and can be used toward whatever they want / need .                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              1\n",
              "['Sanders agrees to participate in Fox News presidential town hall without Clinton', 'I think Bernie Sanders would make a good President.'                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        1\n",
              "and using my recently already acquired video editing skills posted on myself aims for such a career change. doing an oral # presentation blog assignment on # dental healtheconomics is as much fun as it sounds [UNK] https : / / t. net co / 1lkhvyupie                                                                                                                                                                                                                                                                                                                                                                                                                         1\n",
              "Name: tweet, Length: 19985, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "Rx6akGQLkdSi",
        "outputId": "4cbcf397-5bc9-425e-c13b-114bf4c88aa9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-5e3382cc-145e-4357-8f85-1e9c41168c7e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>sarcastic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>the biggest only problem thing i got from coll...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>the absolutely only thing i got fired from the...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>perhaps the second only nice thing i got out f...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>i love it when college professors randomly dra...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>i really love it funny when professors constan...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19981</th>\n",
              "      <td>['8-9ft man found in ancient indian burial mou...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19982</th>\n",
              "      <td>[\"Second Scottish independence referendum 'on ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19983</th>\n",
              "      <td>['Pinoy Cyborg by James Simmons', 'Mag-ingat s...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19984</th>\n",
              "      <td>['The logic here is flawless!', \"No it isn't, ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19985</th>\n",
              "      <td>['TIL One of the founding members of Greenpeac...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>19986 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5e3382cc-145e-4357-8f85-1e9c41168c7e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5e3382cc-145e-4357-8f85-1e9c41168c7e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5e3382cc-145e-4357-8f85-1e9c41168c7e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                   tweet  sarcastic\n",
              "0      the biggest only problem thing i got from coll...          1\n",
              "1      the absolutely only thing i got fired from the...          1\n",
              "2      perhaps the second only nice thing i got out f...          1\n",
              "3      i love it when college professors randomly dra...          1\n",
              "4      i really love it funny when professors constan...          1\n",
              "...                                                  ...        ...\n",
              "19981  ['8-9ft man found in ancient indian burial mou...          0\n",
              "19982  [\"Second Scottish independence referendum 'on ...          0\n",
              "19983  ['Pinoy Cyborg by James Simmons', 'Mag-ingat s...          0\n",
              "19984  ['The logic here is flawless!', \"No it isn't, ...          0\n",
              "19985  ['TIL One of the founding members of Greenpeac...          0\n",
              "\n",
              "[19986 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"sem18(train+test)and sem22(train with data aug)+(13k).csv\")"
      ],
      "metadata": {
        "id": "SXioIschsQnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Cleaning"
      ],
      "metadata": {
        "id": "x1-uZIG0246d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clkFG-g1LS_e",
        "outputId": "16e64a7f-9db6-445e-b02d-2baa0f7fb498"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords  ## stopwords from nltk corpus\n",
        "import nltk\n",
        "b = list(df[\"tweet\"])\n",
        "nltk.download('wordnet')\n",
        "import nltk.corpus\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop = stopwords.words('english')\n",
        "import string\n",
        "exclude = set(string.punctuation)\n",
        "corpus = []\n",
        "for i in range(len(b)):                         # use of regular expression to clean the data\n",
        "    review =re.sub(r'http\\S+', ' ', str(b[i]))\n",
        "    review = re.sub(\"\\d*\\.\\d+\",\"\",review)\n",
        "    review =re.sub(r'@\\S+', ' ', review)\n",
        "    \n",
        "    TAG_RE = re.compile(r'<[^>]+>')\n",
        "    review = TAG_RE.sub('', review)\n",
        "\n",
        "    review = re.sub('\\[[^]]*\\]', ' ', review)\n",
        "    \n",
        "    review = review.lower()\n",
        "    review = review.split()\n",
        "    \n",
        "    review = ' '.join(review)\n",
        "\n",
        "    \n",
        "\n",
        "    \n",
        "\n",
        "    corpus.append(review)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APY0U2xiMLuS"
      },
      "outputs": [],
      "source": [
        "df = df.assign(clean_headlines = corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "DlY6XbOQMqTT",
        "outputId": "fe21f2cb-29ff-4934-94d7-20eebba6e13b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-3dac8cda-a9ed-4742-a02c-9fa9451872c2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>sarcastic</th>\n",
              "      <th>clean_headlines</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>the biggest only problem thing i got from coll...</td>\n",
              "      <td>1</td>\n",
              "      <td>the biggest only problem thing i got from coll...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>the absolutely only thing i got fired from the...</td>\n",
              "      <td>1</td>\n",
              "      <td>the absolutely only thing i got fired from the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>perhaps the second only nice thing i got out f...</td>\n",
              "      <td>1</td>\n",
              "      <td>perhaps the second only nice thing i got out f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>i love it when college professors randomly dra...</td>\n",
              "      <td>1</td>\n",
              "      <td>i love it when college professors randomly dra...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>i really love it funny when professors constan...</td>\n",
              "      <td>1</td>\n",
              "      <td>i really love it funny when professors constan...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19981</th>\n",
              "      <td>['8-9ft man found in ancient indian burial mou...</td>\n",
              "      <td>0</td>\n",
              "      <td>['8-9ft man found in ancient indian burial mou...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19982</th>\n",
              "      <td>[\"Second Scottish independence referendum 'on ...</td>\n",
              "      <td>0</td>\n",
              "      <td>[\"second scottish independence referendum 'on ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19983</th>\n",
              "      <td>['Pinoy Cyborg by James Simmons', 'Mag-ingat s...</td>\n",
              "      <td>0</td>\n",
              "      <td>['pinoy cyborg by james simmons', 'mag-ingat s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19984</th>\n",
              "      <td>['The logic here is flawless!', \"No it isn't, ...</td>\n",
              "      <td>0</td>\n",
              "      <td>['the logic here is flawless!', \"no it isn't, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19985</th>\n",
              "      <td>['TIL One of the founding members of Greenpeac...</td>\n",
              "      <td>0</td>\n",
              "      <td>['til one of the founding members of greenpeac...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>19986 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3dac8cda-a9ed-4742-a02c-9fa9451872c2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3dac8cda-a9ed-4742-a02c-9fa9451872c2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3dac8cda-a9ed-4742-a02c-9fa9451872c2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                   tweet  ...                                    clean_headlines\n",
              "0      the biggest only problem thing i got from coll...  ...  the biggest only problem thing i got from coll...\n",
              "1      the absolutely only thing i got fired from the...  ...  the absolutely only thing i got fired from the...\n",
              "2      perhaps the second only nice thing i got out f...  ...  perhaps the second only nice thing i got out f...\n",
              "3      i love it when college professors randomly dra...  ...  i love it when college professors randomly dra...\n",
              "4      i really love it funny when professors constan...  ...  i really love it funny when professors constan...\n",
              "...                                                  ...  ...                                                ...\n",
              "19981  ['8-9ft man found in ancient indian burial mou...  ...  ['8-9ft man found in ancient indian burial mou...\n",
              "19982  [\"Second Scottish independence referendum 'on ...  ...  [\"second scottish independence referendum 'on ...\n",
              "19983  ['Pinoy Cyborg by James Simmons', 'Mag-ingat s...  ...  ['pinoy cyborg by james simmons', 'mag-ingat s...\n",
              "19984  ['The logic here is flawless!', \"No it isn't, ...  ...  ['the logic here is flawless!', \"no it isn't, ...\n",
              "19985  ['TIL One of the founding members of Greenpeac...  ...  ['til one of the founding members of greenpeac...\n",
              "\n",
              "[19986 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GuE5BqICAne2"
      },
      "outputs": [],
      "source": [
        "# Create sentence and label lists\n",
        "sentences = df.clean_headlines.values\n",
        "\n",
        "# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
        "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
        "labels = df.sarcastic.values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ex5O1eV-Pfct"
      },
      "source": [
        "## Inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTREubVNFiz4"
      },
      "source": [
        "Next, import the BERT tokenizer, used to convert our text into tokens that correspond to BERT's vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-PniouibtahT",
        "outputId": "08c7a43a-5375-4813-b676-265414518f91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytorch-transformers in /usr/local/lib/python3.7/dist-packages (1.2.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (1.20.46)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (4.62.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (0.0.47)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (2019.12.20)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (1.10.0+cu111)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (0.1.96)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.0->pytorch-transformers) (3.10.0.2)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-transformers) (0.10.0)\n",
            "Requirement already satisfied: botocore<1.24.0,>=1.23.46 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-transformers) (1.23.46)\n",
            "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-transformers) (0.5.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.24.0,>=1.23.46->boto3->pytorch-transformers) (2.8.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.7/dist-packages (from botocore<1.24.0,>=1.23.46->boto3->pytorch-transformers) (1.25.11)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.24.0,>=1.23.46->boto3->pytorch-transformers) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-transformers) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->pytorch-transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->pytorch-transformers) (1.1.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.16.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.10.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.47)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.25.11)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch-transformers\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODCKPuvMtVwF"
      },
      "outputs": [],
      "source": [
        "from pytorch_transformers import XLNetModel, XLNetTokenizer, XLNetForSequenceClassification\n",
        "from pytorch_transformers import AdamW\n",
        "\n",
        "from tqdm import tqdm, trange"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z474sSC6oe7A",
        "outputId": "30500e91-fad6-44e3-fce6-d654de061a78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenize the first sentence:\n",
            "['[CLS]', 'the', 'biggest', 'only', 'problem', 'thing', 'i', 'got', 'from', 'college', 'is', 'a', 'strong', 'caf', '##fe', '##ine', 'heroin', 'addiction', '[SEP]']\n"
          ]
        }
      ],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "print (\"Tokenize the first sentence:\")\n",
        "print (tokenized_texts[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cp9BPRd1tMIo"
      },
      "outputs": [],
      "source": [
        "# Set the maximum sequence length. The longest sequence in our training set is 47, but we'll leave room on the end anyway. \n",
        "# In the original paper, the authors used a length of 512.\n",
        "MAX_LEN = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZeXeNXgo0iQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e499c9a-05b7-434a-b338-14bb842be454"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (549 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (556 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (573 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (982 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (574 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (556 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (521 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (785 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (620 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (547 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (750 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (532 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (744 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (517 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1035 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (663 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1053 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (526 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (850 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (519 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (914 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (608 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (933 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (520 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (522 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (761 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (652 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1058 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (565 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (596 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (581 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (533 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (735 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (540 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (597 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (610 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (528 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (891 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (816 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1060 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (636 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (547 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (912 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (597 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (562 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (712 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (722 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (834 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (600 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (716 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (614 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (593 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (835 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (679 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (551 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (565 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (756 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (549 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (649 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1012 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (548 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1051 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (592 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (564 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (923 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (653 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (551 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (601 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (670 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1302 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (860 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (774 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (707 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (549 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (838 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (521 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (556 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (753 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (653 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (912 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (564 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (529 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (584 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (657 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (815 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (571 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (635 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1363 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (673 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (516 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (823 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (965 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (875 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (513 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (603 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (630 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (686 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (674 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (894 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (902 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (572 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (597 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1156 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (565 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (571 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (584 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (530 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (809 > 512). Running this sequence through BERT will result in indexing errors\n"
          ]
        }
      ],
      "source": [
        "# Pad our input tokens\n",
        "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
        "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mFhowDMohU4H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2512cef4-3b44-4405-dc65-16f5b6dba659"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (549 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (556 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (573 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (982 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (574 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (556 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (521 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (785 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (620 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (547 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (750 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (532 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (744 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (517 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1035 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (663 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1053 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (526 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (850 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (519 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (914 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (608 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (933 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (520 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (522 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (761 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (652 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1058 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (565 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (596 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (581 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (533 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (735 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (540 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (597 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (610 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (528 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (891 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (816 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1060 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (636 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (547 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (912 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (597 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (562 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (712 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (722 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (834 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (600 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (716 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (614 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (593 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (835 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (679 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (551 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (565 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (756 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (549 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (649 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1012 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (548 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1051 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (592 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (564 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (923 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (653 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (551 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (601 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (670 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1302 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (860 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (774 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (707 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (549 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (838 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (521 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (556 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (753 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (653 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (912 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (564 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (529 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (584 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (657 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (815 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (571 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (635 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1363 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (673 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (516 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (823 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (965 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (875 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (513 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (603 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (630 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (686 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (674 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (894 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (902 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (572 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (597 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1156 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (565 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (571 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (584 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (530 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (809 > 512). Running this sequence through BERT will result in indexing errors\n"
          ]
        }
      ],
      "source": [
        "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
        "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDs-MYtYH8sL"
      },
      "outputs": [],
      "source": [
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhGulL1pExCT"
      },
      "source": [
        "Create the attention masks "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDoC24LeEv3N"
      },
      "outputs": [],
      "source": [
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFbE-UHvsb7-"
      },
      "outputs": [],
      "source": [
        "# Use train_test_split to split our data into train and validation sets for training\n",
        "\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
        "                                                            random_state=2018, test_size=0.1)\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
        "                                             random_state=2018, test_size=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jw5K2A5Ko1RF"
      },
      "outputs": [],
      "source": [
        "# Convert all of our data into torch tensors, the required datatype for our model\n",
        "\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GEgLpFVlo1Z-"
      },
      "outputs": [],
      "source": [
        "# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\n",
        "batch_size = 32\n",
        "\n",
        "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
        "# with an iterator the entire dataset does not need to be loaded into memory\n",
        "\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNl8khAhPYju"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFsCTp_mporB",
        "outputId": "c0ef7508-c5a0-4be0-cc8f-fcb9bd54aa7d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): BertLayerNorm()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top. \n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QxSMw0FrptiL"
      },
      "outputs": [],
      "source": [
        "param_optimizer = list(model.named_parameters())  ## parameter optimizer\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.0}\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLs72DuMODJO",
        "outputId": "5debe68b-bf87-4d85-b5d2-22f0929b9cab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "t_total value of -1 results in schedule not being applied\n"
          ]
        }
      ],
      "source": [
        "# This variable contains all of the hyperparemeter information our training loop needs\n",
        "optimizer = BertAdam(optimizer_grouped_parameters,\n",
        "                     lr=2e-5,\n",
        "                     warmup=.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cQNvaZ9bnyy"
      },
      "outputs": [],
      "source": [
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBzobghA22uD"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6J-FYdx6nFE_",
        "outputId": "0663ce48-4813-4094-a4b9-9ad59337d5a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:   0%|          | 0/3 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/pytorch_pretrained_bert/optimization.py:275: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)\n",
            "  next_m.mul_(beta1).add_(1 - beta1, grad)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.6350376592753836\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  33%|███▎      | 1/3 [04:42<09:24, 282.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.6971560846560847\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      0.67      0.68       963\n",
            "           1       0.70      0.73      0.71      1036\n",
            "\n",
            "    accuracy                           0.70      1999\n",
            "   macro avg       0.70      0.70      0.70      1999\n",
            "weighted avg       0.70      0.70      0.70      1999\n",
            "\n",
            "Train loss: 0.47513949413176754\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  67%|██████▋   | 2/3 [09:23<04:41, 281.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.7201719576719576\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.80      0.73       963\n",
            "           1       0.78      0.64      0.70      1036\n",
            "\n",
            "    accuracy                           0.72      1999\n",
            "   macro avg       0.73      0.72      0.72      1999\n",
            "weighted avg       0.73      0.72      0.72      1999\n",
            "\n",
            "Train loss: 0.29126106904320037\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 100%|██████████| 3/3 [14:04<00:00, 281.66s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.7085648148148148\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.58      0.66       963\n",
            "           1       0.68      0.82      0.75      1036\n",
            "\n",
            "    accuracy                           0.71      1999\n",
            "   macro avg       0.72      0.70      0.70      1999\n",
            "weighted avg       0.72      0.71      0.70      1999\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Store our loss and accuracy for plotting\n",
        "train_loss_set = []\n",
        "\n",
        "# Number of training epochs (recommend between 2 and 4)\n",
        "epochs = 3\n",
        "\n",
        "# trange is a tqdm wrapper around the normal python range\n",
        "for _ in trange(epochs, desc=\"Epoch\"):\n",
        "  \n",
        "  \n",
        "  # Training\n",
        "  \n",
        "  # Set our model to training mode (as opposed to evaluation mode)\n",
        "  model.train()\n",
        "  \n",
        "  # Tracking variables\n",
        "  tr_loss = 0\n",
        "  nb_tr_examples, nb_tr_steps = 0, 0\n",
        "  \n",
        "  # Train the data for one epoch\n",
        "  for step, batch in enumerate(train_dataloader):\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    # Clear out the gradients (by default they accumulate)\n",
        "    optimizer.zero_grad()\n",
        "    # Forward pass\n",
        "    loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "    train_loss_set.append(loss.item())    \n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    # Update parameters and take a step using the computed gradient\n",
        "    optimizer.step()\n",
        "    \n",
        "    \n",
        "    # Update tracking variables\n",
        "    tr_loss += loss.item()\n",
        "    nb_tr_examples += b_input_ids.size(0)\n",
        "    nb_tr_steps += 1\n",
        "\n",
        "  print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
        "    \n",
        "    \n",
        "  # Validation\n",
        "\n",
        "  # Put model in evaluation mode to evaluate loss on the validation set\n",
        "  model.eval()\n",
        "\n",
        "  # Tracking variables \n",
        "  eval_loss, eval_accuracy = 0, 0\n",
        "  nb_eval_steps, nb_eval_examples = 0, 0\n",
        "  predictions , true_labels = [], []\n",
        "\n",
        "  # Evaluate data for one epoch\n",
        "  for batch in validation_dataloader:\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
        "    with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "    \n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "    \n",
        "    eval_accuracy += tmp_eval_accuracy\n",
        "    nb_eval_steps += 1\n",
        "    predictions.append(logits)\n",
        "    true_labels.append(label_ids)\n",
        "    flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "    flat_predictions = np.argmax(flat_predictions, axis=1).flatten()    \n",
        "    flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
        "  \n",
        "  print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
        "  print(classification_report(flat_true_labels, flat_predictions))\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyRa-5CcHv_g"
      },
      "source": [
        "## Training Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-G03mmwH3aI"
      },
      "source": [
        "Let's take a look at our training loss over all batches:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68xreA9JAmG5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "outputId": "ef4ac6e8-9dcf-4564-ea02-73e2467d8374"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.figure(figsize=(15,8))\n",
        "plt.title(\"Training loss\")\n",
        "plt.xlabel(\"Batch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.plot(train_loss_set)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkyubuJSOzg3"
      },
      "source": [
        "##Prediction on test data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tv1YjMIUXdaG"
      },
      "outputs": [],
      "source": [
        "df1 = pd.read_csv(\"train.En.csv\")  # train data for sub-task A "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H1azBJVEX4UK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 658
        },
        "outputId": "e712537b-a2e9-45af-c84d-b89a687f85b5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-42417bce-1ec6-4981-a035-b684e1fb746d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>tweet</th>\n",
              "      <th>sarcastic</th>\n",
              "      <th>rephrase</th>\n",
              "      <th>sarcasm</th>\n",
              "      <th>irony</th>\n",
              "      <th>satire</th>\n",
              "      <th>understatement</th>\n",
              "      <th>overstatement</th>\n",
              "      <th>rhetorical_question</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>The only thing I got from college is a caffein...</td>\n",
              "      <td>1</td>\n",
              "      <td>College is really difficult, expensive, tiring...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>I love it when professors draw a big question ...</td>\n",
              "      <td>1</td>\n",
              "      <td>I do not like when professors don’t write out ...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Remember the hundred emails from companies whe...</td>\n",
              "      <td>1</td>\n",
              "      <td>I, at the bare minimum, wish companies actuall...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Today my pop-pop told me I was not “forced” to...</td>\n",
              "      <td>1</td>\n",
              "      <td>Today my pop-pop told me I was not \"forced\" to...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>@VolphanCarol @littlewhitty @mysticalmanatee I...</td>\n",
              "      <td>1</td>\n",
              "      <td>I would say Ted Cruz is an asshole and doesn’t...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3463</th>\n",
              "      <td>3463</td>\n",
              "      <td>The population spike in Chicago in 9 months is...</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3464</th>\n",
              "      <td>3464</td>\n",
              "      <td>You'd think in the second to last English clas...</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3465</th>\n",
              "      <td>3465</td>\n",
              "      <td>I’m finally surfacing after a holiday to Scotl...</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3466</th>\n",
              "      <td>3466</td>\n",
              "      <td>Couldn't be prouder today. Well done to every ...</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3467</th>\n",
              "      <td>3467</td>\n",
              "      <td>Overheard as my 13 year old games with a frien...</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3468 rows × 10 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-42417bce-1ec6-4981-a035-b684e1fb746d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-42417bce-1ec6-4981-a035-b684e1fb746d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-42417bce-1ec6-4981-a035-b684e1fb746d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      Unnamed: 0  ... rhetorical_question\n",
              "0              0  ...                 0.0\n",
              "1              1  ...                 0.0\n",
              "2              2  ...                 0.0\n",
              "3              3  ...                 0.0\n",
              "4              4  ...                 0.0\n",
              "...          ...  ...                 ...\n",
              "3463        3463  ...                 NaN\n",
              "3464        3464  ...                 NaN\n",
              "3465        3465  ...                 NaN\n",
              "3466        3466  ...                 NaN\n",
              "3467        3467  ...                 NaN\n",
              "\n",
              "[3468 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "df1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Cleaning for test data "
      ],
      "metadata": {
        "id": "qHqx2mkz7KE7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CClscCrjXgqF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1acb7048-fdcc-4db9-fdc3-43e2ecfcf68e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import re                                                       ### Data Cleaning for test data \n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "ps = PorterStemmer()\n",
        "\n",
        "\n",
        "b = list(df1[\"tweet\"])\n",
        "\n",
        "\n",
        "wordnet=WordNetLemmatizer()\n",
        "nltk.download('wordnet')\n",
        "import nltk.corpus\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop = stopwords.words('english')\n",
        "import string\n",
        "exclude = set(string.punctuation)\n",
        "corpus = []\n",
        "for i in range(len(b)):\n",
        "    review =re.sub(r'http\\S+', ' ', str(b[i]))\n",
        "    review = re.sub(\"\\d*\\.\\d+\",\"\",review)\n",
        "    review =re.sub(r'@\\S+', ' ', review)\n",
        "    \n",
        "    \n",
        "    review = re.sub('\\[[^]]*\\]', ' ', review)\n",
        "    \n",
        "    review = review.lower()\n",
        "    review = review.split()\n",
        "    \n",
        "    review = ' '.join(review)\n",
        "\n",
        "    \n",
        "\n",
        "    corpus.append(review)\n",
        "df1 = df1.assign(clean_tweet = corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESy_GZ9iX-NM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 832
        },
        "outputId": "6f8e4416-4d75-40d3-8cf6-6f4adbf002ce"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-9ed353fa-f5bb-4e1c-a60f-e6dc10d18e25\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>tweet</th>\n",
              "      <th>sarcastic</th>\n",
              "      <th>rephrase</th>\n",
              "      <th>sarcasm</th>\n",
              "      <th>irony</th>\n",
              "      <th>satire</th>\n",
              "      <th>understatement</th>\n",
              "      <th>overstatement</th>\n",
              "      <th>rhetorical_question</th>\n",
              "      <th>clean_tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>The only thing I got from college is a caffein...</td>\n",
              "      <td>1</td>\n",
              "      <td>College is really difficult, expensive, tiring...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>the only thing i got from college is a caffein...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>I love it when professors draw a big question ...</td>\n",
              "      <td>1</td>\n",
              "      <td>I do not like when professors don’t write out ...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>i love it when professors draw a big question ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Remember the hundred emails from companies whe...</td>\n",
              "      <td>1</td>\n",
              "      <td>I, at the bare minimum, wish companies actuall...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>remember the hundred emails from companies whe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Today my pop-pop told me I was not “forced” to...</td>\n",
              "      <td>1</td>\n",
              "      <td>Today my pop-pop told me I was not \"forced\" to...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>today my pop-pop told me i was not “forced” to...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>@VolphanCarol @littlewhitty @mysticalmanatee I...</td>\n",
              "      <td>1</td>\n",
              "      <td>I would say Ted Cruz is an asshole and doesn’t...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>i did too, and i also reported cancun cruz not...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3463</th>\n",
              "      <td>3463</td>\n",
              "      <td>The population spike in Chicago in 9 months is...</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>the population spike in chicago in 9 months is...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3464</th>\n",
              "      <td>3464</td>\n",
              "      <td>You'd think in the second to last English clas...</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>you'd think in the second to last english clas...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3465</th>\n",
              "      <td>3465</td>\n",
              "      <td>I’m finally surfacing after a holiday to Scotl...</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>i’m finally surfacing after a holiday to scotl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3466</th>\n",
              "      <td>3466</td>\n",
              "      <td>Couldn't be prouder today. Well done to every ...</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>couldn't be prouder today. well done to every ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3467</th>\n",
              "      <td>3467</td>\n",
              "      <td>Overheard as my 13 year old games with a frien...</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>overheard as my 13 year old games with a frien...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3468 rows × 11 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9ed353fa-f5bb-4e1c-a60f-e6dc10d18e25')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9ed353fa-f5bb-4e1c-a60f-e6dc10d18e25 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9ed353fa-f5bb-4e1c-a60f-e6dc10d18e25');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      Unnamed: 0  ...                                        clean_tweet\n",
              "0              0  ...  the only thing i got from college is a caffein...\n",
              "1              1  ...  i love it when professors draw a big question ...\n",
              "2              2  ...  remember the hundred emails from companies whe...\n",
              "3              3  ...  today my pop-pop told me i was not “forced” to...\n",
              "4              4  ...  i did too, and i also reported cancun cruz not...\n",
              "...          ...  ...                                                ...\n",
              "3463        3463  ...  the population spike in chicago in 9 months is...\n",
              "3464        3464  ...  you'd think in the second to last english clas...\n",
              "3465        3465  ...  i’m finally surfacing after a holiday to scotl...\n",
              "3466        3466  ...  couldn't be prouder today. well done to every ...\n",
              "3467        3467  ...  overheard as my 13 year old games with a frien...\n",
              "\n",
              "[3468 rows x 11 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "df1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T19Byk_4YJHn"
      },
      "outputs": [],
      "source": [
        "df1 = df1.drop([\"rephrase\",\"sarcasm\",\"irony\",\"satire\",\"understatement\",\"overstatement\",\"rhetorical_question\",\"Unnamed: 0\",\"tweet\"],axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ec5b3lFYTXX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "19a2cf8f-e830-425b-ac11-fad2725125f6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-4133f0c0-5275-4029-a3b8-b4c7ff7d115b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sarcastic</th>\n",
              "      <th>clean_tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>the only thing i got from college is a caffein...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>i love it when professors draw a big question ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>remember the hundred emails from companies whe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>today my pop-pop told me i was not “forced” to...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>i did too, and i also reported cancun cruz not...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3463</th>\n",
              "      <td>0</td>\n",
              "      <td>the population spike in chicago in 9 months is...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3464</th>\n",
              "      <td>0</td>\n",
              "      <td>you'd think in the second to last english clas...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3465</th>\n",
              "      <td>0</td>\n",
              "      <td>i’m finally surfacing after a holiday to scotl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3466</th>\n",
              "      <td>0</td>\n",
              "      <td>couldn't be prouder today. well done to every ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3467</th>\n",
              "      <td>0</td>\n",
              "      <td>overheard as my 13 year old games with a frien...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3468 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4133f0c0-5275-4029-a3b8-b4c7ff7d115b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4133f0c0-5275-4029-a3b8-b4c7ff7d115b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4133f0c0-5275-4029-a3b8-b4c7ff7d115b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      sarcastic                                        clean_tweet\n",
              "0             1  the only thing i got from college is a caffein...\n",
              "1             1  i love it when professors draw a big question ...\n",
              "2             1  remember the hundred emails from companies whe...\n",
              "3             1  today my pop-pop told me i was not “forced” to...\n",
              "4             1  i did too, and i also reported cancun cruz not...\n",
              "...         ...                                                ...\n",
              "3463          0  the population spike in chicago in 9 months is...\n",
              "3464          0  you'd think in the second to last english clas...\n",
              "3465          0  i’m finally surfacing after a holiday to scotl...\n",
              "3466          0  couldn't be prouder today. well done to every ...\n",
              "3467          0  overheard as my 13 year old games with a frien...\n",
              "\n",
              "[3468 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "df1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAN0LZBOOPVh"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Create sentence and label lists\n",
        "sentences = df1.clean_tweet.values\n",
        "\n",
        "# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
        "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
        "labels = df1.sarcastic.values\n",
        "\n",
        "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "\n",
        "\n",
        "MAX_LEN = 128\n",
        "# Pad our input tokens\n",
        "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
        "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
        "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask) \n",
        "\n",
        "prediction_inputs = torch.tensor(input_ids)\n",
        "prediction_masks = torch.tensor(attention_masks)\n",
        "prediction_labels = torch.tensor(labels)\n",
        "  \n",
        "batch_size = 32  \n",
        "\n",
        "\n",
        "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hba10sXR7Xi6"
      },
      "outputs": [],
      "source": [
        "# Prediction on test set\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n",
        "  with torch.no_grad():\n",
        "    # Forward pass, calculate logit predictions\n",
        "    logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cRaZQ4XC7kLs"
      },
      "outputs": [],
      "source": [
        "# Import and evaluate each test batch using Matthew's correlation coefficient\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "matthews_set = []\n",
        "\n",
        "for i in range(len(true_labels)):\n",
        "  matthews = matthews_corrcoef(true_labels[i],\n",
        "                 np.argmax(predictions[i], axis=1).flatten())\n",
        "  matthews_set.append(matthews)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUM0UA1qJaVB"
      },
      "source": [
        "The final score will be based on the entire test set, but let's take a look at the scores on the individual batches to get a sense of the variability in the metric between batches.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xytAr_C48wnu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b77cef93-85f5-4ad6-eec9-e87b93e50795"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0]"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "matthews_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCYZa1lQ8Jn8"
      },
      "outputs": [],
      "source": [
        "# Flatten the predictions and true values for aggregate Matthew's evaluation on the whole dataset\n",
        "flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "flat_true_labels = [item for sublist in true_labels for item in sublist]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-4EBZ8D8VsZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c491b5bd-223c-4e66-e775-5cf564200a03"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.92116793611515"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "matthews_corrcoef(flat_true_labels, flat_predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YxBidf8f3v5p"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "msp4Foo43nhh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e266e2f8-1f96-49b5-9af0-9e7a6962fb44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98      2601\n",
            "           1       0.92      0.97      0.94       867\n",
            "\n",
            "    accuracy                           0.97      3468\n",
            "   macro avg       0.95      0.97      0.96      3468\n",
            "weighted avg       0.97      0.97      0.97      3468\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(flat_true_labels, flat_predictions))   # classification report "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNQx1AXs4N-t"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sb-c5mPM4POV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "199517cc-8743-450d-83ca-e333fd06383b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9603087013803664"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "f1_score(flat_true_labels, flat_predictions, average='macro')  # macro F1-score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YXrk7oz2ZZ_s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70e769d8-85f7-4822-f0fc-9bbba9b9c902"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, ..., 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ],
      "source": [
        "flat_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08bz11N1Z7gB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5de9f05-f109-4524-de8f-bd4ce7605a23"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2526,   75],\n",
              "       [  30,  837]])"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix(flat_true_labels, flat_predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKhHhoz4w_Y8"
      },
      "outputs": [],
      "source": [
        "test_data = pd.read_csv(\"taskA.En.input.csv\")    # test data for sub-task A  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3BILwAkx1GQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "57d6beeb-ca26-4ec0-af26-b432344b1deb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-417d108c-e568-4a68-8302-48ce6994f1e5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Size on the the Toulouse team, That pack is mo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Pinball!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>So the Scottish Government want people to get ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>villainous pro tip : change the device name on...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I would date any of these men 🥺</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1395</th>\n",
              "      <td>I’ve just seen this and felt it deserved a Ret...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1396</th>\n",
              "      <td>Omg how an earth is that a pen !!! 🤡</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1397</th>\n",
              "      <td>Bringing Kanye and drake to a tl near you</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1398</th>\n",
              "      <td>I love it when women are referred to as \"girl ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1399</th>\n",
              "      <td>The fact that people still don't get that you ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1400 rows × 1 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-417d108c-e568-4a68-8302-48ce6994f1e5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-417d108c-e568-4a68-8302-48ce6994f1e5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-417d108c-e568-4a68-8302-48ce6994f1e5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                   text\n",
              "0     Size on the the Toulouse team, That pack is mo...\n",
              "1                                              Pinball!\n",
              "2     So the Scottish Government want people to get ...\n",
              "3     villainous pro tip : change the device name on...\n",
              "4                       I would date any of these men 🥺\n",
              "...                                                 ...\n",
              "1395  I’ve just seen this and felt it deserved a Ret...\n",
              "1396               Omg how an earth is that a pen !!! 🤡\n",
              "1397          Bringing Kanye and drake to a tl near you\n",
              "1398  I love it when women are referred to as \"girl ...\n",
              "1399  The fact that people still don't get that you ...\n",
              "\n",
              "[1400 rows x 1 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ],
      "source": [
        "test_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-10XM0r8xqUf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "526782b3-e5d5-4dde-86eb-94a1f8694fd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "ps = PorterStemmer()\n",
        "\n",
        "\n",
        "b = list(test_data[\"text\"])\n",
        "\n",
        "\n",
        "wordnet=WordNetLemmatizer()\n",
        "nltk.download('wordnet')\n",
        "import nltk.corpus\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop = stopwords.words('english')\n",
        "import string\n",
        "exclude = set(string.punctuation)\n",
        "corpus = []\n",
        "for i in range(len(b)):\n",
        "    review =re.sub(r'http\\S+', ' ', str(b[i]))\n",
        "    review = re.sub(\"\\d*\\.\\d+\",\"\",review)\n",
        "    review =re.sub(r'@\\S+', ' ', review)\n",
        "    \n",
        "    \n",
        "    review = re.sub('\\[[^]]*\\]', ' ', review)\n",
        "    \n",
        "    review = review.lower()\n",
        "    review = review.split()\n",
        "    \n",
        "    review = ' '.join(review)\n",
        "\n",
        "    \n",
        "\n",
        "    corpus.append(review)\n",
        "test_data = test_data.assign(clean_text = corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swYraRFDyRO0"
      },
      "outputs": [],
      "source": [
        "test_data = test_data.assign(random_label=[0 for i in range(len(test_data[\"clean_text\"]))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbDvHbcfyhfq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "4a34d93f-ae4d-4427-bb26-3f6758bbd5f7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-48e36c38-4bdf-45da-a4a4-c2adf57e9cf3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>clean_text</th>\n",
              "      <th>random_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Size on the the Toulouse team, That pack is mo...</td>\n",
              "      <td>size on the the toulouse team, that pack is mo...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Pinball!</td>\n",
              "      <td>pinball!</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>So the Scottish Government want people to get ...</td>\n",
              "      <td>so the scottish government want people to get ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>villainous pro tip : change the device name on...</td>\n",
              "      <td>villainous pro tip : change the device name on...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I would date any of these men 🥺</td>\n",
              "      <td>i would date any of these men 🥺</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1395</th>\n",
              "      <td>I’ve just seen this and felt it deserved a Ret...</td>\n",
              "      <td>i’ve just seen this and felt it deserved a ret...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1396</th>\n",
              "      <td>Omg how an earth is that a pen !!! 🤡</td>\n",
              "      <td>omg how an earth is that a pen !!! 🤡</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1397</th>\n",
              "      <td>Bringing Kanye and drake to a tl near you</td>\n",
              "      <td>bringing kanye and drake to a tl near you</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1398</th>\n",
              "      <td>I love it when women are referred to as \"girl ...</td>\n",
              "      <td>i love it when women are referred to as \"girl ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1399</th>\n",
              "      <td>The fact that people still don't get that you ...</td>\n",
              "      <td>the fact that people still don't get that you ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1400 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-48e36c38-4bdf-45da-a4a4-c2adf57e9cf3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-48e36c38-4bdf-45da-a4a4-c2adf57e9cf3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-48e36c38-4bdf-45da-a4a4-c2adf57e9cf3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                   text  ... random_label\n",
              "0     Size on the the Toulouse team, That pack is mo...  ...            0\n",
              "1                                              Pinball!  ...            0\n",
              "2     So the Scottish Government want people to get ...  ...            0\n",
              "3     villainous pro tip : change the device name on...  ...            0\n",
              "4                       I would date any of these men 🥺  ...            0\n",
              "...                                                 ...  ...          ...\n",
              "1395  I’ve just seen this and felt it deserved a Ret...  ...            0\n",
              "1396               Omg how an earth is that a pen !!! 🤡  ...            0\n",
              "1397          Bringing Kanye and drake to a tl near you  ...            0\n",
              "1398  I love it when women are referred to as \"girl ...  ...            0\n",
              "1399  The fact that people still don't get that you ...  ...            0\n",
              "\n",
              "[1400 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ],
      "source": [
        "test_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tAg6Z5LmyKDL"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Create sentence and label lists\n",
        "sentences = test_data.clean_text.values\n",
        "\n",
        "# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
        "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
        "labels = test_data.random_label.values\n",
        "\n",
        "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "\n",
        "\n",
        "MAX_LEN = 128\n",
        "# Pad our input tokens\n",
        "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
        "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
        "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask) \n",
        "\n",
        "prediction_inputs = torch.tensor(input_ids)\n",
        "prediction_masks = torch.tensor(attention_masks)\n",
        "prediction_labels = torch.tensor(labels)\n",
        "  \n",
        "batch_size = 32  \n",
        "\n",
        "\n",
        "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6plBf36x-kW"
      },
      "outputs": [],
      "source": [
        "# Prediction on test set\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n",
        "  with torch.no_grad():\n",
        "    # Forward pass, calculate logit predictions\n",
        "    logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wIxxG7ciyrar"
      },
      "outputs": [],
      "source": [
        "flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "flat_true_labels = [item for sublist in true_labels for item in sublist]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BxYGa6Bfy7Kn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4272c843-25d1-4886-aa40-b7bf4117c71c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 1, ..., 1, 1, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ],
      "source": [
        "flat_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89OleVEzy93Z"
      },
      "outputs": [],
      "source": [
        "test_data = test_data.assign(predicted_label=list(flat_predictions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p5-bBkLlzS2k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "b2155f35-8741-4dd3-8251-2e8b186f02c4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-e8e13671-016a-4842-b4fc-ea4bc9c9013b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>clean_text</th>\n",
              "      <th>random_label</th>\n",
              "      <th>predicted_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Size on the the Toulouse team, That pack is mo...</td>\n",
              "      <td>size on the the toulouse team, that pack is mo...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Pinball!</td>\n",
              "      <td>pinball!</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>So the Scottish Government want people to get ...</td>\n",
              "      <td>so the scottish government want people to get ...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>villainous pro tip : change the device name on...</td>\n",
              "      <td>villainous pro tip : change the device name on...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I would date any of these men 🥺</td>\n",
              "      <td>i would date any of these men 🥺</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1395</th>\n",
              "      <td>I’ve just seen this and felt it deserved a Ret...</td>\n",
              "      <td>i’ve just seen this and felt it deserved a ret...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1396</th>\n",
              "      <td>Omg how an earth is that a pen !!! 🤡</td>\n",
              "      <td>omg how an earth is that a pen !!! 🤡</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1397</th>\n",
              "      <td>Bringing Kanye and drake to a tl near you</td>\n",
              "      <td>bringing kanye and drake to a tl near you</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1398</th>\n",
              "      <td>I love it when women are referred to as \"girl ...</td>\n",
              "      <td>i love it when women are referred to as \"girl ...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1399</th>\n",
              "      <td>The fact that people still don't get that you ...</td>\n",
              "      <td>the fact that people still don't get that you ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1400 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e8e13671-016a-4842-b4fc-ea4bc9c9013b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e8e13671-016a-4842-b4fc-ea4bc9c9013b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e8e13671-016a-4842-b4fc-ea4bc9c9013b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                   text  ... predicted_label\n",
              "0     Size on the the Toulouse team, That pack is mo...  ...               1\n",
              "1                                              Pinball!  ...               0\n",
              "2     So the Scottish Government want people to get ...  ...               1\n",
              "3     villainous pro tip : change the device name on...  ...               0\n",
              "4                       I would date any of these men 🥺  ...               0\n",
              "...                                                 ...  ...             ...\n",
              "1395  I’ve just seen this and felt it deserved a Ret...  ...               0\n",
              "1396               Omg how an earth is that a pen !!! 🤡  ...               0\n",
              "1397          Bringing Kanye and drake to a tl near you  ...               1\n",
              "1398  I love it when women are referred to as \"girl ...  ...               1\n",
              "1399  The fact that people still don't get that you ...  ...               0\n",
              "\n",
              "[1400 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ],
      "source": [
        "test_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wd8-Bj2TzXYS"
      },
      "outputs": [],
      "source": [
        "test_data = test_data.drop([\"text\",\"clean_text\",\"random_label\"],axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ggjw4cdszpYy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6689a420-601b-468c-a98a-2c4b9efff470"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    911\n",
              "1    489\n",
              "Name: predicted_label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ],
      "source": [
        "test_data[\"predicted_label\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6MKC96QT0gD6"
      },
      "outputs": [],
      "source": [
        "test_data.to_csv(\"task_a_final.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction on test data "
      ],
      "metadata": {
        "id": "0LE5134x4kH2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sem18 = pd.read_csv(\"taskC.En.input.csv\")"
      ],
      "metadata": {
        "id": "ZgyJFi0-87My"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sem18   # test data which contain two column text_o & text_1 "
      ],
      "metadata": {
        "id": "_HQCtTfj9NYi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "5b7ca35d-a190-4cce-fc97-98448d64db1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-e6ca88d2-e6af-49f6-b443-073d5ceb5e44\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_0</th>\n",
              "      <th>text_1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I see that your team played well today!</td>\n",
              "      <td>I'm sorry that your team didn't win yesterday.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Anthony Taylor is such a fair referee, I wish ...</td>\n",
              "      <td>I hope Anthony Taylor is never put in charge o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>the weather is gloomy, just raining and dull.</td>\n",
              "      <td>What a glorious weather today</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>People going out to get there boosters without...</td>\n",
              "      <td>Nice to see the sheep getting their boosters t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Really great weather we're having, love a bit ...</td>\n",
              "      <td>Really cold January so far - looking forward t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>195</th>\n",
              "      <td>the tories betrayed the nation, what a surprise!</td>\n",
              "      <td>the tories betrayed the nation, as expected</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>196</th>\n",
              "      <td>Cant believe we have to spend the rest of our ...</td>\n",
              "      <td>Cant wait to spend the rest of my life waiting...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>197</th>\n",
              "      <td>Isn't it just amazing how competent the govern...</td>\n",
              "      <td>Everything is a total mess, how can anyone be ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>198</th>\n",
              "      <td>Thanks Boris Johnson for restricting travel ab...</td>\n",
              "      <td>The reasoning behind the tightening of travel ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199</th>\n",
              "      <td>Where is my invite to the tories Christmas par...</td>\n",
              "      <td>The Tory Christmas party scandal is a shame to...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>200 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e6ca88d2-e6af-49f6-b443-073d5ceb5e44')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e6ca88d2-e6af-49f6-b443-073d5ceb5e44 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e6ca88d2-e6af-49f6-b443-073d5ceb5e44');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                text_0                                             text_1\n",
              "0             I see that your team played well today!      I'm sorry that your team didn't win yesterday.\n",
              "1    Anthony Taylor is such a fair referee, I wish ...  I hope Anthony Taylor is never put in charge o...\n",
              "2       the weather is gloomy, just raining and dull.                      What a glorious weather today \n",
              "3    People going out to get there boosters without...  Nice to see the sheep getting their boosters t...\n",
              "4    Really great weather we're having, love a bit ...  Really cold January so far - looking forward t...\n",
              "..                                                 ...                                                ...\n",
              "195   the tories betrayed the nation, what a surprise!        the tories betrayed the nation, as expected\n",
              "196  Cant believe we have to spend the rest of our ...  Cant wait to spend the rest of my life waiting...\n",
              "197  Isn't it just amazing how competent the govern...  Everything is a total mess, how can anyone be ...\n",
              "198  Thanks Boris Johnson for restricting travel ab...  The reasoning behind the tightening of travel ...\n",
              "199  Where is my invite to the tories Christmas par...  The Tory Christmas party scandal is a shame to...\n",
              "\n",
              "[200 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Data cleaning for test data of Sub-Task C"
      ],
      "metadata": {
        "id": "3H5tlZu57w-D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "ps = PorterStemmer()\n",
        "\n",
        "\n",
        "b = list(sem18[\"text_0\"])\n",
        "\n",
        "\n",
        "wordnet=WordNetLemmatizer()\n",
        "nltk.download('wordnet')\n",
        "import nltk.corpus\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop = stopwords.words('english')\n",
        "import string\n",
        "exclude = set(string.punctuation)\n",
        "corpus = []\n",
        "for i in range(len(b)):\n",
        "    review =re.sub(r'http\\S+', ' ', str(b[i]))\n",
        "    review = re.sub(\"\\d*\\.\\d+\",\"\",review)\n",
        "    review =re.sub(r'@\\S+', ' ', review)\n",
        "    \n",
        "    TAG_RE = re.compile(r'<[^>]+>')\n",
        "    review = TAG_RE.sub('', review)\n",
        "    \n",
        "    review = re.sub('\\[[^]]*\\]', ' ', review)\n",
        "    \n",
        "    review = review.lower()\n",
        "    review = review.split()\n",
        "    \n",
        "    review = ' '.join(review)\n",
        "\n",
        "    \n",
        "\n",
        "    corpus.append(review)\n",
        "sem18 = sem18.assign(clean_text0 = corpus)\n",
        "\n",
        "\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "ps = PorterStemmer()\n",
        "\n",
        "\n",
        "b = list(sem18[\"text_1\"])\n",
        "\n",
        "\n",
        "wordnet=WordNetLemmatizer()\n",
        "nltk.download('wordnet')\n",
        "import nltk.corpus\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop = stopwords.words('english')\n",
        "import string\n",
        "exclude = set(string.punctuation)\n",
        "corpus = []\n",
        "for i in range(len(b)):\n",
        "    review =re.sub(r'http\\S+', ' ', str(b[i]))\n",
        "    review = re.sub(\"\\d*\\.\\d+\",\"\",review)\n",
        "    review =re.sub(r'@\\S+', ' ', review)\n",
        "    \n",
        "    TAG_RE = re.compile(r'<[^>]+>')\n",
        "    review = TAG_RE.sub('', review)\n",
        "    \n",
        "    review = re.sub('\\[[^]]*\\]', ' ', review)\n",
        "    \n",
        "    review = review.lower()\n",
        "    review = review.split()\n",
        "    \n",
        "    review = ' '.join(review)\n",
        "\n",
        "    \n",
        "\n",
        "    corpus.append(review)\n",
        "sem18 = sem18.assign(clean_text1 = corpus)"
      ],
      "metadata": {
        "id": "05RETEADSWl7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "682f3170-c5aa-4722-dc52-ad9858fa8778"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sem18"
      ],
      "metadata": {
        "id": "obr4wSoOTNDw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        },
        "outputId": "b969712b-6129-4208-a8ef-824c052a3f71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-81e0eb7d-b3b4-4e14-8e82-4e6ca020d607\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_0</th>\n",
              "      <th>text_1</th>\n",
              "      <th>clean_text0</th>\n",
              "      <th>clean_text1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I see that your team played well today!</td>\n",
              "      <td>I'm sorry that your team didn't win yesterday.</td>\n",
              "      <td>i see that your team played well today!</td>\n",
              "      <td>i'm sorry that your team didn't win yesterday.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Anthony Taylor is such a fair referee, I wish ...</td>\n",
              "      <td>I hope Anthony Taylor is never put in charge o...</td>\n",
              "      <td>anthony taylor is such a fair referee, i wish ...</td>\n",
              "      <td>i hope anthony taylor is never put in charge o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>the weather is gloomy, just raining and dull.</td>\n",
              "      <td>What a glorious weather today</td>\n",
              "      <td>the weather is gloomy, just raining and dull.</td>\n",
              "      <td>what a glorious weather today</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>People going out to get there boosters without...</td>\n",
              "      <td>Nice to see the sheep getting their boosters t...</td>\n",
              "      <td>people going out to get there boosters without...</td>\n",
              "      <td>nice to see the sheep getting their boosters t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Really great weather we're having, love a bit ...</td>\n",
              "      <td>Really cold January so far - looking forward t...</td>\n",
              "      <td>really great weather we're having, love a bit ...</td>\n",
              "      <td>really cold january so far - looking forward t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>195</th>\n",
              "      <td>the tories betrayed the nation, what a surprise!</td>\n",
              "      <td>the tories betrayed the nation, as expected</td>\n",
              "      <td>the tories betrayed the nation, what a surprise!</td>\n",
              "      <td>the tories betrayed the nation, as expected</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>196</th>\n",
              "      <td>Cant believe we have to spend the rest of our ...</td>\n",
              "      <td>Cant wait to spend the rest of my life waiting...</td>\n",
              "      <td>cant believe we have to spend the rest of our ...</td>\n",
              "      <td>cant wait to spend the rest of my life waiting...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>197</th>\n",
              "      <td>Isn't it just amazing how competent the govern...</td>\n",
              "      <td>Everything is a total mess, how can anyone be ...</td>\n",
              "      <td>isn't it just amazing how competent the govern...</td>\n",
              "      <td>everything is a total mess, how can anyone be ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>198</th>\n",
              "      <td>Thanks Boris Johnson for restricting travel ab...</td>\n",
              "      <td>The reasoning behind the tightening of travel ...</td>\n",
              "      <td>thanks boris johnson for restricting travel ab...</td>\n",
              "      <td>the reasoning behind the tightening of travel ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199</th>\n",
              "      <td>Where is my invite to the tories Christmas par...</td>\n",
              "      <td>The Tory Christmas party scandal is a shame to...</td>\n",
              "      <td>where is my invite to the tories christmas par...</td>\n",
              "      <td>the tory christmas party scandal is a shame to...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>200 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-81e0eb7d-b3b4-4e14-8e82-4e6ca020d607')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-81e0eb7d-b3b4-4e14-8e82-4e6ca020d607 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-81e0eb7d-b3b4-4e14-8e82-4e6ca020d607');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                text_0  ...                                        clean_text1\n",
              "0             I see that your team played well today!   ...     i'm sorry that your team didn't win yesterday.\n",
              "1    Anthony Taylor is such a fair referee, I wish ...  ...  i hope anthony taylor is never put in charge o...\n",
              "2       the weather is gloomy, just raining and dull.   ...                      what a glorious weather today\n",
              "3    People going out to get there boosters without...  ...  nice to see the sheep getting their boosters t...\n",
              "4    Really great weather we're having, love a bit ...  ...  really cold january so far - looking forward t...\n",
              "..                                                 ...  ...                                                ...\n",
              "195   the tories betrayed the nation, what a surprise!  ...        the tories betrayed the nation, as expected\n",
              "196  Cant believe we have to spend the rest of our ...  ...  cant wait to spend the rest of my life waiting...\n",
              "197  Isn't it just amazing how competent the govern...  ...  everything is a total mess, how can anyone be ...\n",
              "198  Thanks Boris Johnson for restricting travel ab...  ...  the reasoning behind the tightening of travel ...\n",
              "199  Where is my invite to the tories Christmas par...  ...  the tory christmas party scandal is a shame to...\n",
              "\n",
              "[200 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "taskc = sem18"
      ],
      "metadata": {
        "id": "s8rgmaKFfIXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "taskc = taskc.assign(random_label=[1 for i in range(len(taskc[\"clean_text0\"]))])"
      ],
      "metadata": {
        "id": "i2pDOqjrfN7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "taskc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "id": "vMAYkE5Efb8D",
        "outputId": "f2474388-e3c5-47f4-c440-c7f1cd3ced64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-535a6dcb-6714-4f6a-88e3-8f7d11ce8d80\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_0</th>\n",
              "      <th>text_1</th>\n",
              "      <th>clean_text0</th>\n",
              "      <th>clean_text1</th>\n",
              "      <th>random_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I see that your team played well today!</td>\n",
              "      <td>I'm sorry that your team didn't win yesterday.</td>\n",
              "      <td>i see that your team played well today!</td>\n",
              "      <td>i'm sorry that your team didn't win yesterday.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Anthony Taylor is such a fair referee, I wish ...</td>\n",
              "      <td>I hope Anthony Taylor is never put in charge o...</td>\n",
              "      <td>anthony taylor is such a fair referee, i wish ...</td>\n",
              "      <td>i hope anthony taylor is never put in charge o...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>the weather is gloomy, just raining and dull.</td>\n",
              "      <td>What a glorious weather today</td>\n",
              "      <td>the weather is gloomy, just raining and dull.</td>\n",
              "      <td>what a glorious weather today</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>People going out to get there boosters without...</td>\n",
              "      <td>Nice to see the sheep getting their boosters t...</td>\n",
              "      <td>people going out to get there boosters without...</td>\n",
              "      <td>nice to see the sheep getting their boosters t...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Really great weather we're having, love a bit ...</td>\n",
              "      <td>Really cold January so far - looking forward t...</td>\n",
              "      <td>really great weather we're having, love a bit ...</td>\n",
              "      <td>really cold january so far - looking forward t...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>195</th>\n",
              "      <td>the tories betrayed the nation, what a surprise!</td>\n",
              "      <td>the tories betrayed the nation, as expected</td>\n",
              "      <td>the tories betrayed the nation, what a surprise!</td>\n",
              "      <td>the tories betrayed the nation, as expected</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>196</th>\n",
              "      <td>Cant believe we have to spend the rest of our ...</td>\n",
              "      <td>Cant wait to spend the rest of my life waiting...</td>\n",
              "      <td>cant believe we have to spend the rest of our ...</td>\n",
              "      <td>cant wait to spend the rest of my life waiting...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>197</th>\n",
              "      <td>Isn't it just amazing how competent the govern...</td>\n",
              "      <td>Everything is a total mess, how can anyone be ...</td>\n",
              "      <td>isn't it just amazing how competent the govern...</td>\n",
              "      <td>everything is a total mess, how can anyone be ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>198</th>\n",
              "      <td>Thanks Boris Johnson for restricting travel ab...</td>\n",
              "      <td>The reasoning behind the tightening of travel ...</td>\n",
              "      <td>thanks boris johnson for restricting travel ab...</td>\n",
              "      <td>the reasoning behind the tightening of travel ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199</th>\n",
              "      <td>Where is my invite to the tories Christmas par...</td>\n",
              "      <td>The Tory Christmas party scandal is a shame to...</td>\n",
              "      <td>where is my invite to the tories christmas par...</td>\n",
              "      <td>the tory christmas party scandal is a shame to...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>200 rows × 5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-535a6dcb-6714-4f6a-88e3-8f7d11ce8d80')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-535a6dcb-6714-4f6a-88e3-8f7d11ce8d80 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-535a6dcb-6714-4f6a-88e3-8f7d11ce8d80');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                text_0  ... random_label\n",
              "0             I see that your team played well today!   ...            1\n",
              "1    Anthony Taylor is such a fair referee, I wish ...  ...            1\n",
              "2       the weather is gloomy, just raining and dull.   ...            1\n",
              "3    People going out to get there boosters without...  ...            1\n",
              "4    Really great weather we're having, love a bit ...  ...            1\n",
              "..                                                 ...  ...          ...\n",
              "195   the tories betrayed the nation, what a surprise!  ...            1\n",
              "196  Cant believe we have to spend the rest of our ...  ...            1\n",
              "197  Isn't it just amazing how competent the govern...  ...            1\n",
              "198  Thanks Boris Johnson for restricting travel ab...  ...            1\n",
              "199  Where is my invite to the tories Christmas par...  ...            1\n",
              "\n",
              "[200 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Create sentence and label lists\n",
        "sentences = taskc.clean_text0.values\n",
        "\n",
        "# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
        "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
        "labels = taskc.random_label.values\n",
        "\n",
        "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "\n",
        "\n",
        "MAX_LEN = 128\n",
        "# Pad our input tokens\n",
        "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
        "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
        "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask) \n",
        "\n",
        "prediction_inputs = torch.tensor(input_ids)\n",
        "prediction_masks = torch.tensor(attention_masks)\n",
        "prediction_labels = torch.tensor(labels)\n",
        "  \n",
        "batch_size = 32  \n",
        "\n",
        "\n",
        "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n",
        "\n",
        "# Prediction on test set\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n",
        "  with torch.no_grad():\n",
        "    # Forward pass, calculate logit predictions\n",
        "    logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "\n",
        "# Flatten the predictions and true values for aggregate Matthew's evaluation on the whole dataset\n",
        "flat_predictions0 = [np.max(item) for sublist in predictions for item in sublist]\n",
        "#flat_predictions0 = np.argmax(flat_predictions0, axis=1).flatten()\n",
        "#flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
        "\n",
        "#print(classification_report(flat_true_labels, flat_predictions))\n"
      ],
      "metadata": {
        "id": "2X-WvJU48gga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flat_predictions0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2uMiYUqdjml",
        "outputId": "7dcab723-3e6d-4c55-9183-4679c2838582"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.7608713,\n",
              " 0.7080813,\n",
              " 1.706366,\n",
              " 0.15219535,\n",
              " 1.5920824,\n",
              " 1.1219702,\n",
              " 0.6854785,\n",
              " 1.7624915,\n",
              " 1.9944491,\n",
              " 1.4736637,\n",
              " 2.323301,\n",
              " 2.2847092,\n",
              " 0.43609494,\n",
              " 1.6007919,\n",
              " 1.9566172,\n",
              " 0.63546205,\n",
              " 0.74461955,\n",
              " 1.658704,\n",
              " 0.96399975,\n",
              " 0.82162267,\n",
              " 3.1465971,\n",
              " 0.46826658,\n",
              " 1.1068212,\n",
              " 1.5555677,\n",
              " 1.31186,\n",
              " 3.0520391,\n",
              " 1.8120421,\n",
              " 1.8269125,\n",
              " 1.1635934,\n",
              " 3.3196397,\n",
              " 2.434213,\n",
              " 1.3402886,\n",
              " 1.3657967,\n",
              " 2.5562239,\n",
              " 1.1562792,\n",
              " 1.7933794,\n",
              " 1.8260792,\n",
              " 0.17328171,\n",
              " 0.39557788,\n",
              " 0.6207615,\n",
              " 2.202608,\n",
              " 0.33053026,\n",
              " 2.5247989,\n",
              " 3.0279028,\n",
              " 0.27091256,\n",
              " 2.1829696,\n",
              " 0.34092072,\n",
              " 1.583185,\n",
              " 1.233443,\n",
              " 1.0987856,\n",
              " 2.5932965,\n",
              " 0.4980228,\n",
              " -0.024251407,\n",
              " 1.4970628,\n",
              " 0.763232,\n",
              " 1.2059063,\n",
              " 1.6458799,\n",
              " 2.1793153,\n",
              " 1.7064028,\n",
              " 3.3610067,\n",
              " 0.162246,\n",
              " 1.7355548,\n",
              " 1.6160496,\n",
              " 0.18319374,\n",
              " 1.363882,\n",
              " 2.009463,\n",
              " 1.5163296,\n",
              " 0.8032914,\n",
              " 1.8956419,\n",
              " 3.1878734,\n",
              " 2.4562051,\n",
              " 0.99606067,\n",
              " 1.4184942,\n",
              " 1.0046315,\n",
              " 2.1023464,\n",
              " 1.5441043,\n",
              " 0.4998901,\n",
              " 3.9285288,\n",
              " 1.1495519,\n",
              " 1.2540494,\n",
              " 0.026416037,\n",
              " 0.06649044,\n",
              " 2.8134413,\n",
              " 0.7513791,\n",
              " 1.9384333,\n",
              " 0.29820734,\n",
              " 2.1892743,\n",
              " 1.6750813,\n",
              " 0.20753177,\n",
              " 3.1536252,\n",
              " -0.03599223,\n",
              " 1.729261,\n",
              " 3.72656,\n",
              " 1.2397431,\n",
              " 0.65439,\n",
              " 1.1935612,\n",
              " 0.095767,\n",
              " 1.6830944,\n",
              " 1.7566719,\n",
              " 1.9116174,\n",
              " 0.47815356,\n",
              " 1.7589947,\n",
              " 0.3171479,\n",
              " 1.6445173,\n",
              " 0.32938233,\n",
              " 0.80325234,\n",
              " 0.8645418,\n",
              " 1.548043,\n",
              " 2.4494758,\n",
              " 0.5771116,\n",
              " 0.67365825,\n",
              " 0.9007973,\n",
              " 3.2220404,\n",
              " 0.11822587,\n",
              " 0.04936545,\n",
              " 0.8666499,\n",
              " 2.7151222,\n",
              " 2.4173362,\n",
              " 2.26889,\n",
              " 1.5465952,\n",
              " 0.0928985,\n",
              " 1.2572736,\n",
              " 0.84202325,\n",
              " 1.8146144,\n",
              " 1.9379154,\n",
              " 1.3443207,\n",
              " 1.6815512,\n",
              " 2.1631784,\n",
              " 1.9523497,\n",
              " 1.1666032,\n",
              " 2.5115871,\n",
              " 0.24386092,\n",
              " 0.9452368,\n",
              " 0.7282672,\n",
              " 0.08525159,\n",
              " 2.0942328,\n",
              " 1.6182498,\n",
              " 4.2350287,\n",
              " 0.11338124,\n",
              " 0.70676726,\n",
              " 1.1501545,\n",
              " 0.4009501,\n",
              " 1.6411136,\n",
              " 1.0844197,\n",
              " -0.061433427,\n",
              " 1.6288363,\n",
              " 0.28158584,\n",
              " 2.133295,\n",
              " 1.9657217,\n",
              " 1.7371914,\n",
              " 1.5745736,\n",
              " 1.4406971,\n",
              " 0.21592888,\n",
              " 1.0940493,\n",
              " 0.96766245,\n",
              " 1.7057847,\n",
              " 3.830124,\n",
              " 1.5522331,\n",
              " 0.7423665,\n",
              " 0.91604537,\n",
              " 0.27683136,\n",
              " 0.02905325,\n",
              " 3.646337,\n",
              " 0.03781708,\n",
              " 0.18718545,\n",
              " 3.9539623,\n",
              " 2.107225,\n",
              " 0.6103683,\n",
              " 1.8130497,\n",
              " 2.9693425,\n",
              " 0.9025592,\n",
              " 0.0728562,\n",
              " 1.0905911,\n",
              " 1.567462,\n",
              " 1.5982606,\n",
              " 1.2138958,\n",
              " 3.4990163,\n",
              " 4.114738,\n",
              " 1.7375258,\n",
              " 0.46677876,\n",
              " 4.100039,\n",
              " 2.6573794,\n",
              " 2.9221697,\n",
              " 1.8653677,\n",
              " 2.5823216,\n",
              " 3.5937982,\n",
              " 0.73519886,\n",
              " 2.2693021,\n",
              " 0.20766015,\n",
              " 1.1038554,\n",
              " 0.49571785,\n",
              " 1.3363628,\n",
              " 0.25543728,\n",
              " 1.1517186,\n",
              " 0.122588575,\n",
              " 2.2486858,\n",
              " 1.2027477,\n",
              " 2.478776,\n",
              " 3.7691274,\n",
              " 0.95340115]"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Create sentence and label lists\n",
        "sentences = taskc.clean_text1.values\n",
        "\n",
        "# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
        "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
        "labels = taskc.random_label.values\n",
        "\n",
        "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "\n",
        "\n",
        "MAX_LEN = 128\n",
        "# Pad our input tokens\n",
        "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
        "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
        "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask) \n",
        "\n",
        "prediction_inputs = torch.tensor(input_ids)\n",
        "prediction_masks = torch.tensor(attention_masks)\n",
        "prediction_labels = torch.tensor(labels)\n",
        "  \n",
        "batch_size = 32  \n",
        "\n",
        "\n",
        "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n",
        "\n",
        "# Prediction on test set\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n",
        "  with torch.no_grad():\n",
        "    # Forward pass, calculate logit predictions\n",
        "    logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "\n",
        "# Flatten the predictions and true values for aggregate Matthew's evaluation on the whole dataset\n",
        "flat_predictions1 = [np.max(item) for sublist in predictions for item in sublist]\n",
        "#flat_predictions0 = np.argmax(flat_predictions0, axis=1).flatten()\n",
        "#flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
        "\n",
        "#print(classification_report(flat_true_labels, flat_predictions))\n"
      ],
      "metadata": {
        "id": "HD0vbqvch6Ay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flat_predictions1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWJ3Ccr7iFSu",
        "outputId": "b47a1d3d-69d7-491c-b289-2f076a103260"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.4679077,\n",
              " 1.4400802,\n",
              " 2.6561399,\n",
              " 3.367429,\n",
              " 1.3386084,\n",
              " 1.5227206,\n",
              " 1.0479062,\n",
              " 1.1448731,\n",
              " 0.69405824,\n",
              " 1.7005635,\n",
              " 2.5672622,\n",
              " 3.4995508,\n",
              " 2.3198347,\n",
              " 2.3247402,\n",
              " 1.9379154,\n",
              " 1.3287985,\n",
              " 0.8931776,\n",
              " 0.5484964,\n",
              " 1.6489308,\n",
              " 1.5164059,\n",
              " 3.128333,\n",
              " 1.5314883,\n",
              " 1.404349,\n",
              " 1.7164679,\n",
              " 1.5334486,\n",
              " 1.7915276,\n",
              " 0.6608538,\n",
              " 0.54348123,\n",
              " 1.8510358,\n",
              " 1.1739231,\n",
              " 0.43176135,\n",
              " 3.0608797,\n",
              " 2.1665251,\n",
              " 1.6038399,\n",
              " 0.40297428,\n",
              " 1.9705648,\n",
              " 2.571014,\n",
              " 1.128703,\n",
              " 0.73039913,\n",
              " 1.7825264,\n",
              " 2.99332,\n",
              " 0.52142197,\n",
              " 0.20927204,\n",
              " 0.5062396,\n",
              " 1.8319026,\n",
              " 2.38039,\n",
              " 1.1229845,\n",
              " 3.4388268,\n",
              " 0.15976065,\n",
              " 1.0813674,\n",
              " 1.2143278,\n",
              " 3.3059874,\n",
              " 3.4912477,\n",
              " 1.8972281,\n",
              " 1.2683961,\n",
              " 2.7143078,\n",
              " 1.1711962,\n",
              " 1.1655042,\n",
              " 0.8808457,\n",
              " 2.0150483,\n",
              " 0.1544349,\n",
              " 1.9614111,\n",
              " 0.20587264,\n",
              " 0.35184857,\n",
              " 0.9585289,\n",
              " 1.8625627,\n",
              " 0.152833,\n",
              " 2.3907464,\n",
              " 1.4349378,\n",
              " 1.1432397,\n",
              " 0.61627936,\n",
              " 1.1487327,\n",
              " 1.7137473,\n",
              " 2.1910179,\n",
              " 1.0764598,\n",
              " 1.7068506,\n",
              " 0.24799852,\n",
              " 1.7704645,\n",
              " 0.63463515,\n",
              " 0.47715095,\n",
              " 0.38354936,\n",
              " 3.802936,\n",
              " 0.50915056,\n",
              " 3.3612418,\n",
              " 2.3469224,\n",
              " 2.9716833,\n",
              " 2.9132442,\n",
              " 0.73239166,\n",
              " 1.8971545,\n",
              " 1.5401086,\n",
              " 1.2130431,\n",
              " 0.45692295,\n",
              " 1.6626482,\n",
              " 3.5013947,\n",
              " 0.20658721,\n",
              " 0.06869052,\n",
              " 0.37948474,\n",
              " 0.29617736,\n",
              " 3.3720417,\n",
              " 0.85868233,\n",
              " 3.0424323,\n",
              " 1.2236565,\n",
              " 0.6235625,\n",
              " 0.58475673,\n",
              " 1.3519102,\n",
              " 1.7868575,\n",
              " 2.3748047,\n",
              " 3.0585945,\n",
              " 0.24759994,\n",
              " 1.4912113,\n",
              " 0.70616543,\n",
              " 0.010032851,\n",
              " 1.6235821,\n",
              " 0.27553555,\n",
              " 1.0776259,\n",
              " 0.87961894,\n",
              " 1.1981798,\n",
              " 3.3172908,\n",
              " 0.10488668,\n",
              " 2.1316776,\n",
              " 1.5648297,\n",
              " 2.339874,\n",
              " 2.882607,\n",
              " 0.055277772,\n",
              " 1.9566172,\n",
              " 1.1897186,\n",
              " 1.0064477,\n",
              " 0.47497672,\n",
              " 2.7986407,\n",
              " 0.6523933,\n",
              " 0.6224411,\n",
              " 0.04737261,\n",
              " 3.7494183,\n",
              " 1.9944156,\n",
              " 0.17908926,\n",
              " 1.0795455,\n",
              " 0.4419088,\n",
              " 0.594176,\n",
              " 0.30490103,\n",
              " 1.5096594,\n",
              " 0.878503,\n",
              " 1.6016223,\n",
              " 0.42968357,\n",
              " 3.3771782,\n",
              " 0.99251395,\n",
              " 2.024381,\n",
              " 0.0040627657,\n",
              " 1.7970964,\n",
              " 2.1428468,\n",
              " 0.17701516,\n",
              " 2.7205186,\n",
              " 1.0712472,\n",
              " 0.4782245,\n",
              " 0.57226366,\n",
              " 1.2797912,\n",
              " 0.51600355,\n",
              " 1.7692567,\n",
              " 2.4271512,\n",
              " -0.0037921057,\n",
              " 2.6077862,\n",
              " 1.4110038,\n",
              " 1.3242054,\n",
              " 1.199194,\n",
              " 0.01131101,\n",
              " 0.90292233,\n",
              " 1.3442901,\n",
              " 0.018354306,\n",
              " 3.1117535,\n",
              " 1.7350576,\n",
              " 0.32354477,\n",
              " 1.0819021,\n",
              " 1.0645033,\n",
              " 3.9697185,\n",
              " 2.3416386,\n",
              " 1.0345628,\n",
              " 2.012282,\n",
              " 0.5669767,\n",
              " 0.95889443,\n",
              " 0.7262513,\n",
              " 0.68189996,\n",
              " 1.9989227,\n",
              " 0.21848087,\n",
              " 0.9719646,\n",
              " 0.02530967,\n",
              " 2.0909781,\n",
              " 0.22294433,\n",
              " 0.40887263,\n",
              " 1.5241842,\n",
              " 2.548358,\n",
              " 2.1314614,\n",
              " 1.0779831,\n",
              " 2.679093,\n",
              " 0.35008916,\n",
              " 2.157922,\n",
              " 0.5041985,\n",
              " 0.64798445,\n",
              " 1.4702791,\n",
              " 0.72499985,\n",
              " 1.0845174,\n",
              " 0.62955254]"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "task_c_en = []\n",
        "for i in range(200):\n",
        "  if flat_predictions0[i]>flat_predictions1[i]:\n",
        "    task_c_en.append(0)\n",
        "  if flat_predictions0[i]<flat_predictions1[i]:\n",
        "    task_c_en.append(1)\n",
        "task_c_en"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7fTX1NMibwR",
        "outputId": "032a0825-cac0-4d87-e9b2-eed8398b8613"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0]"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_last = pd.DataFrame(task_c_en,columns=[\"task_c_en\"])"
      ],
      "metadata": {
        "id": "p__HAofgkEG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_last"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "FVASCDO_kWFv",
        "outputId": "37f48118-4d0a-461c-ccc7-380f8107bb25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-708eae80-70d4-4fff-9745-64a22e73f00b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>task_c_en</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>195</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>196</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>197</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>198</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>200 rows × 1 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-708eae80-70d4-4fff-9745-64a22e73f00b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-708eae80-70d4-4fff-9745-64a22e73f00b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-708eae80-70d4-4fff-9745-64a22e73f00b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     task_c_en\n",
              "0            0\n",
              "1            1\n",
              "2            1\n",
              "3            1\n",
              "4            0\n",
              "..         ...\n",
              "195          0\n",
              "196          1\n",
              "197          0\n",
              "198          0\n",
              "199          0\n",
              "\n",
              "[200 rows x 1 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_last[\"task_c_en\"].value_counts()   # value counts of predicted labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVFhKBofqF3h",
        "outputId": "8e3c9cba-85f5-4421-b3d0-b77667263146"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    106\n",
              "0     94\n",
              "Name: task_c_en, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_last.to_csv(\"task_c_en.csv\")"
      ],
      "metadata": {
        "id": "GBQIAQ5rkcfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfjYoa6WmkN6"
      },
      "source": [
        "## Conclusion"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "BERT_Fine_Tuning_Sentence_Classification.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
